#Run with dummy inputs:
batch_size = 1, sequence_length = 128

(groqflow) swimzebra@groq-r01-gn-08:~/groqflow/proof_points/natural_language_processing/bert$ python bert_tiny.py
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████| 346/346 [00:00<00:00, 4.07MB/s]
vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 6.56MB/s]
special_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 1.75MB/s]
config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████| 760/760 [00:00<00:00, 10.3MB/s]
pytorch_model.bin: 100%|██████████████████████████████████████████████████████████████████████████████| 17.6M/17.6M [00:00<00:00, 120MB/s]
/home/swimzebra/miniconda3/envs/groqflow/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()



Building "bert_tiny"
    ✓ Exporting PyTorch to ONNX
    ✓ Optimizing ONNX file
    ✓ Checking for Op support
    ✓ Converting to FP16
    ✓ Compiling model
    ✓ Assembling model

Woohoo! Saved to ~/.cache/groqflow/bert_tiny
Preprocessing data.
/home/swimzebra/miniconda3/envs/groqflow/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Downloading builder script: 100%|████████████████████████████████████████████████████████████████████| 9.13k/9.13k [00:00<00:00, 35.7MB/s]
Downloading readme: 100%|████████████████████████████████████████████████████████████████████████████| 6.68k/6.68k [00:00<00:00, 34.8MB/s]
Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████| 6.37M/6.37M [00:01<00:00, 5.15MB/s]
Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████| 790k/790k [00:00<00:00, 1.51MB/s]
Generating train split: 100%|███████████████████████████████████████████████████████████████| 8544/8544 [00:00<00:00, 11163.25 examples/s]
Generating validation split: 100%|███████████████████████████████████████████████████████████| 1101/1101 [00:00<00:00, 2032.19 examples/s]
Generating test split: 100%|█████████████████████████████████████████████████████████████████| 2210/2210 [00:00<00:00, 3963.05 examples/s]

Info: No inputs received for benchmark. Using the inputs provided during model compilation.
Running inference on GroqChip.
Running inference using PyTorch model (CPU).
100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2210/2210 [00:04<00:00, 451.76it/s]
+--------+----------+-------------------------+----------------+----------------------+-------------+
| Source | Accuracy | end-to-end latency (ms) | end-to-end IPS | on-chip latency (ms) | on-chip IPS |
+--------+----------+-------------------------+----------------+----------------------+-------------+
|  cpu   |  77.47%  |           2.21          |     451.69     |          --          |      --     |
|  groq  |  77.47%  |           0.07          |    15121.15    |         0.03         |   37576.72  |
+--------+----------+-------------------------+----------------+----------------------+-------------+
Proof point /home/swimzebra/groqflow/proof_points/natural_language_processing/bert/bert_tiny.py finished!


#Run with custom inputs:
batch_size = 1, sequence_length = 256
(groqflow) swimzebra@groq-r01-gn-08:~/groqflow/proof_points/natural_language_processing/bert$ vi bert_tiny.py
(groqflow) swimzebra@groq-r01-gn-08:~/groqflow/proof_points/natural_language_processing/bert$ python bert_tiny.py
/home/swimzebra/miniconda3/envs/groqflow/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Warning: build_model() discovered a cached build of bert_tiny, but decided to rebuild for the following reasons:

         - Input shape of model "bert_tiny" changed from {'attention_mask': (1, 128), 'input_ids': (1, 128)} to {'attention_mask': (1, 256), 'input_ids': (1, 256)} since the last time it was built.

         build_model() will now rebuild your model to ensure correctness. You can change this policy by setting the build_model(rebuild=...) argument.



Building "bert_tiny"
    ✓ Exporting PyTorch to ONNX
    ✓ Optimizing ONNX file
    ✓ Checking for Op support
    ✓ Converting to FP16
    ✓ Compiling model
    ✓ Assembling model

Woohoo! Saved to ~/.cache/groqflow/bert_tiny
Preprocessing data.
/home/swimzebra/miniconda3/envs/groqflow/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(

Info: No inputs received for benchmark. Using the inputs provided during model compilation.
Running inference on GroqChip.
Running inference using PyTorch model (CPU).
100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2210/2210 [00:09<00:00, 244.43it/s]
+--------+----------+-------------------------+----------------+----------------------+-------------+
| Source | Accuracy | end-to-end latency (ms) | end-to-end IPS | on-chip latency (ms) | on-chip IPS |
+--------+----------+-------------------------+----------------+----------------------+-------------+
|  cpu   |  77.47%  |           4.09          |     244.35     |          --          |      --     |
|  groq  |  77.47%  |           0.08          |    12364.87    |         0.04         |   23391.20  |
+--------+----------+-------------------------+----------------+----------------------+-------------+
Proof point /home/swimzebra/groqflow/proof_points/natural_language_processing/bert/bert_tiny.py finished!
