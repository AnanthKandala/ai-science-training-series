{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c22441-db13-4415-a605-a9a1d99ee83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from hw3_helpers import get_data, Classifier, evaluate, train_one_epoch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa15c19-02c0-4af3-88af-c5403b2e35c3",
   "metadata": {},
   "source": [
    "Create training, validation and testing dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4c2f1e-69c1-49d9-9b99-4a302b1552f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1eda371-ddc0-4978-8525-760f34e847c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba52ec7-52a3-4b6a-9c37-8037990bdf4b",
   "metadata": {},
   "source": [
    "Creating the bulding blocks of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a5cafe-8510-4eaa-9be5-413d21eb5bc4",
   "metadata": {},
   "source": [
    "Initialize the model, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f9435-85ec-4450-9449-9a2223e6d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_initial_filters = 64; n_stages = 12; blocks_per_stage = 1\n",
    "model = Classifier(n_initial_filters, n_stages, blocks_per_stage)\n",
    "model.to(dev)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53088d-ab97-4bf8-8453-95deb72e7cbb",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340a98a-e1ba-4cdd-9af2-094571a8f43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 30\n",
    "cols = [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"]\n",
    "training_frame = {col: [] for col in cols}\n",
    "n_step = 2\n",
    "for j in range(epochs):\n",
    "    train_one_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_acc, train_loss = evaluate(train_dataloader, model, loss_fn) \n",
    "    print(f\"Epoch {j}: train loss: {train_loss:.3f}, accuracy: {train_acc:.3f}\" )\n",
    "    val_acc, val_loss = evaluate(val_dataloader, model, loss_fn)\n",
    "    # # scheduler.step(val_loss)\n",
    "    # if j>n_step:\n",
    "    #     # avg_train_los = np.mean(training_frame[\"train_loss\"][-(1+n_step):-1])\n",
    "    #     val_convg = np.max(training_frame[\"val_loss\"][-(1+n_step):-1]) - np.min(training_frame[\"val_loss\"][-(1+n_step):-1])\n",
    "    #     print(val_convg)\n",
    "    #     if val_convg<1e-2:\n",
    "    #         print(f\"decreased lr, lr was {param_group['lr']}\")\n",
    "    #         for param_group in optimizer.param_groups:\n",
    "    #             param_group['lr'] /= 0.1\n",
    "    print(f\"Epoch {j}: validation loss: {val_loss:.3f}, accuracy: {val_acc:.3f}\")\n",
    "    training_frame[\"train_loss\"].append(train_loss), training_frame[\"train_acc\"].append(train_acc)\n",
    "    training_frame[\"val_loss\"].append(val_loss), training_frame[\"val_acc\"].append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5d956-d409-452b-8695-038949e5690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frame = pd.DataFrame(training_frame)\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "title = f\"n_initial_filters: {n_initial_filters}, n_stages: {n_stages}, blocks_per_stage: {blocks_per_stage}\"\n",
    "title += \"\\n conv_filter_size reduced to 7 -> 3, increased initial stride: 1 -> 2, added a lr scheduler\\n\"\n",
    "plt.suptitle(title)\n",
    "training_frame.plot(y=[\"train_loss\", \"val_loss\"], ax=axs[0], xlabel='Epochs', title=\"Loss\")\n",
    "training_frame.plot(y=[\"train_acc\", \"val_acc\"], ax=axs[1], xlabel='Epochs', title=\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show(fig)\n",
    "plot = f'nif={n_initial_filters}_nstg={n_stages}_blks={blocks_per_stage}_lr_schedule'\n",
    "fig.savefig(f'{plot}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca25fc-5e5a-4ef1-94ac-82d0c52ac0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_frame = pd.DataFrame(training_frame)\n",
    "# fig, axs = plt.subplots(ncols=2)\n",
    "# title = f\"n_initial_filters: {n_initial_filters}, n_stages: {n_stages}, blocks_per_stage: {blocks_per_stage}\"\n",
    "# title += \"\\n conv_filter_size reduced to 7 -> 3, increased initial stride: 1 -> 2\"\n",
    "# plt.suptitle(title)\n",
    "# training_frame.plot(y=[\"train_loss\", \"val_loss\"], ax=axs[0], xlabel='Epochs', title=\"Loss\")\n",
    "# training_frame.plot(y=[\"train_acc\", \"val_acc\"], ax=axs[1], xlabel='Epochs', title=\"Accuracy\")\n",
    "# plt.show(fig)\n",
    "# plot = f'nif={n_initial_filters}_nstg={n_stages}_blks={blocks_per_stage}_2'\n",
    "# fig.savefig(f'{plot}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71daafb1-da42-4e31-9db1-9b9aa020eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original model\n",
    "# 64, 4, 2\n",
    "string = \"\"\"These are the results of the original model. \n",
    "Clearly, the model is overfitting as the validation_accuracy has saturated at value smaller than training_accuracy.\"\"\"\n",
    "print(string)\n",
    "Image(\"original_model.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a0b5c-e987-4366-94a5-47e43fa54aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_initial_filers halved, num_stages halved\n",
    "string = \"\"\"To reduce overfitting, we reduce the num_parameters in the model by:\n",
    "1.) Halving the intial_filters from 64 to 32\n",
    "2.) Halving the number of stages from 4 to 2\n",
    "Clearly, the model is still overfitting as the validation_accuracy has saturated at value smaller than training_accuracy.\"\"\"\n",
    "print(string)\n",
    "Image(\"reduced_size_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acce373-4c75-4e3b-b154-3fd2f250dad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience/conda-2023-10-04",
   "language": "python",
   "name": "conda-2023-10-03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
